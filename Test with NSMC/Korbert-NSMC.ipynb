{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification , BertForSequenceClassification, BertConfig ,BertForQuestionAnswering\n",
    "from tokenization import BertTokenizer\n",
    "\n",
    "from transformers import  AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quadro RTX 8000'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/ratings_train.txt', sep='\\t')\n",
    "test_df = pd.read_csv('./data/ratings_test.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "train_df = train_df.sample(frac=0.5, random_state=999)\n",
    "test_df = test_df.sample(frac=0.5, random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_df['document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"./vocab\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 이 영화(제작과정포함)를 접한 후 결론 → 샤론스톤은 쓰레기다. [SEP]\n",
      "['[CLS]', '_', '이_', '영화', '(', '제작', '과정', '포함', ')', '를_', '접', '한_', '후_', '결', '론_', '→', '_', '샤', '론', '스', '톤', '은_', '쓰', '레', '기다', '._', '[SEP]', '_']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,    9,   10, 1041,   15, 1006, 4320,  940,   19,   32,  559,\n",
       "         16,  375,  295, 3018, 3784,    9, 1406,  534,   56, 2498,   18,\n",
       "        479,  193, 1729,    7,    3,    9,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력 토큰의 최대 시퀀스 길이\n",
    "MAX_LEN = 128\n",
    "\n",
    "# 토큰을 숫자 인덱스로 변환\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 마스크 초기화\n",
    "attention_masks = []\n",
    "\n",
    "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
    "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련셋과 검증셋으로 분리\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
    "                                                                                    labels, \n",
    "                                                                                    random_state=0, \n",
    "                                                                                    test_size=0.1)\n",
    "\n",
    "# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
    "                                                       input_ids,\n",
    "                                                       random_state=0, \n",
    "                                                       test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "train_segment = np.zeros((len(train_inputs),128))\n",
    "validation_segment = np.zeros((len(validation_inputs),128))\n",
    "\n",
    "# 데이터를 파이토치의 텐서로 변환\n",
    "train_inputs = torch.tensor(train_inputs,dtype=torch.long)\n",
    "train_labels = torch.tensor(train_labels,dtype=torch.long)\n",
    "train_masks = torch.tensor(train_masks,dtype=torch.long)\n",
    "train_segment = torch.tensor(train_segment,dtype=torch.long)\n",
    "validation_inputs = torch.tensor(validation_inputs,dtype=torch.long)\n",
    "validation_labels = torch.tensor(validation_labels,dtype=torch.long)\n",
    "validation_masks = torch.tensor(validation_masks,dtype=torch.long)\n",
    "validation_segment = torch.tensor(validation_segment,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 사이즈\n",
    "batch_size = 128\n",
    "\n",
    "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
    "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_segment ,train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_segment ,validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"./pretrained_model\",num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # 학습률\n",
    "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
    "                )\n",
    "# 에폭수\n",
    "epochs = 1\n",
    "\n",
    "# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# 학습률을 조금씩 감소시키는 스케줄러 생성\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30797, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/528 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   0%|          | 1/528 [00:01<10:17,  1.17s/it]\u001b[A\n",
      "Iteration:   0%|          | 2/528 [00:02<10:09,  1.16s/it]\u001b[A\n",
      "Iteration:   1%|          | 3/528 [00:03<10:02,  1.15s/it]\u001b[A\n",
      "Iteration:   1%|          | 4/528 [00:04<09:57,  1.14s/it]\u001b[A\n",
      "Iteration:   1%|          | 5/528 [00:05<09:54,  1.14s/it]\u001b[A\n",
      "Iteration:   1%|          | 6/528 [00:06<09:50,  1.13s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 7/528 [00:07<09:49,  1.13s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 8/528 [00:09<09:47,  1.13s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 9/528 [00:10<09:46,  1.13s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 10/528 [00:11<09:46,  1.13s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 11/528 [00:12<09:46,  1.13s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 12/528 [00:13<09:45,  1.14s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 13/528 [00:14<09:44,  1.14s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 14/528 [00:15<09:43,  1.14s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 15/528 [00:17<09:43,  1.14s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 16/528 [00:18<09:43,  1.14s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 17/528 [00:19<09:42,  1.14s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 18/528 [00:20<09:41,  1.14s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 19/528 [00:21<09:40,  1.14s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 20/528 [00:22<09:40,  1.14s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 21/528 [00:23<09:40,  1.15s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 22/528 [00:25<09:40,  1.15s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 23/528 [00:26<09:39,  1.15s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 24/528 [00:27<09:40,  1.15s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 25/528 [00:28<09:39,  1.15s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 26/528 [00:29<09:38,  1.15s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 27/528 [00:30<09:37,  1.15s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 28/528 [00:31<09:36,  1.15s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 29/528 [00:33<09:35,  1.15s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 30/528 [00:34<09:34,  1.15s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 31/528 [00:35<09:34,  1.16s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 32/528 [00:36<09:34,  1.16s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 33/528 [00:37<09:33,  1.16s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 34/528 [00:38<09:32,  1.16s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 35/528 [00:40<09:31,  1.16s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 36/528 [00:41<09:30,  1.16s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 37/528 [00:42<09:29,  1.16s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 38/528 [00:43<09:28,  1.16s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 39/528 [00:44<09:28,  1.16s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 40/528 [00:45<09:28,  1.16s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 41/528 [00:47<09:27,  1.16s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 42/528 [00:48<09:26,  1.16s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 43/528 [00:49<09:24,  1.16s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 44/528 [00:50<09:23,  1.16s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 45/528 [00:51<09:25,  1.17s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 46/528 [00:52<09:24,  1.17s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 47/528 [00:54<09:23,  1.17s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 48/528 [00:55<09:21,  1.17s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 49/528 [00:56<09:19,  1.17s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 50/528 [00:57<09:17,  1.17s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 51/528 [00:58<09:15,  1.17s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 52/528 [00:59<09:15,  1.17s/it]\u001b[A\n",
      "Iteration:  10%|█         | 53/528 [01:01<09:14,  1.17s/it]\u001b[A\n",
      "Iteration:  10%|█         | 54/528 [01:02<09:12,  1.17s/it]\u001b[A\n",
      "Iteration:  10%|█         | 55/528 [01:03<09:11,  1.17s/it]\u001b[A\n",
      "Iteration:  11%|█         | 56/528 [01:04<09:11,  1.17s/it]\u001b[A\n",
      "Iteration:  11%|█         | 57/528 [01:05<09:09,  1.17s/it]\u001b[A\n",
      "Iteration:  11%|█         | 58/528 [01:06<09:07,  1.17s/it]\u001b[A\n",
      "Iteration:  11%|█         | 59/528 [01:08<09:06,  1.17s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 60/528 [01:09<09:05,  1.17s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 61/528 [01:10<09:06,  1.17s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 62/528 [01:11<09:04,  1.17s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 63/528 [01:12<09:02,  1.17s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 64/528 [01:13<09:01,  1.17s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 65/528 [01:15<08:59,  1.17s/it]\u001b[A\n",
      "Iteration:  12%|█▎        | 66/528 [01:16<08:59,  1.17s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 67/528 [01:17<08:57,  1.17s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 68/528 [01:18<08:56,  1.17s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 69/528 [01:19<08:55,  1.17s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 70/528 [01:20<08:54,  1.17s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 71/528 [01:22<08:52,  1.17s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 72/528 [01:23<08:51,  1.17s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 73/528 [01:24<08:50,  1.17s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 74/528 [01:25<08:49,  1.17s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 75/528 [01:26<08:48,  1.17s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 76/528 [01:27<08:47,  1.17s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 77/528 [01:29<08:46,  1.17s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 78/528 [01:30<08:45,  1.17s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 79/528 [01:31<08:46,  1.17s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 80/528 [01:32<08:45,  1.17s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 81/528 [01:33<08:44,  1.17s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 82/528 [01:34<08:43,  1.17s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 83/528 [01:36<08:42,  1.17s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 84/528 [01:37<08:40,  1.17s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 85/528 [01:38<08:38,  1.17s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 86/528 [01:39<08:36,  1.17s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 87/528 [01:40<08:35,  1.17s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 88/528 [01:41<08:35,  1.17s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 89/528 [01:43<08:33,  1.17s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 90/528 [01:44<08:32,  1.17s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 91/528 [01:45<08:31,  1.17s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 92/528 [01:46<08:30,  1.17s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 93/528 [01:47<08:28,  1.17s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 94/528 [01:48<08:27,  1.17s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 95/528 [01:50<08:28,  1.17s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 96/528 [01:51<08:25,  1.17s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 97/528 [01:52<08:23,  1.17s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 98/528 [01:53<08:21,  1.17s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 99/528 [01:54<08:20,  1.17s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 100/528 [01:55<08:19,  1.17s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 101/528 [01:57<08:18,  1.17s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 102/528 [01:58<08:17,  1.17s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 103/528 [01:59<08:15,  1.17s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 104/528 [02:00<08:14,  1.17s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 105/528 [02:01<08:13,  1.17s/it]\u001b[A\n",
      "Iteration:  20%|██        | 106/528 [02:02<08:12,  1.17s/it]\u001b[A\n",
      "Iteration:  20%|██        | 107/528 [02:04<08:11,  1.17s/it]\u001b[A\n",
      "Iteration:  20%|██        | 108/528 [02:05<08:10,  1.17s/it]\u001b[A\n",
      "Iteration:  21%|██        | 109/528 [02:06<08:08,  1.17s/it]\u001b[A\n",
      "Iteration:  21%|██        | 110/528 [02:07<08:07,  1.17s/it]\u001b[A\n",
      "Iteration:  21%|██        | 111/528 [02:08<08:06,  1.17s/it]\u001b[A\n",
      "Iteration:  21%|██        | 112/528 [02:09<08:05,  1.17s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 113/528 [02:11<08:04,  1.17s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 114/528 [02:12<08:02,  1.17s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 115/528 [02:13<08:01,  1.17s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 116/528 [02:14<08:00,  1.17s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 117/528 [02:15<07:59,  1.17s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 118/528 [02:16<07:58,  1.17s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 119/528 [02:18<07:57,  1.17s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 120/528 [02:19<07:56,  1.17s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 121/528 [02:20<07:55,  1.17s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 122/528 [02:21<07:53,  1.17s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 123/528 [02:22<07:52,  1.17s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 124/528 [02:23<07:51,  1.17s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 125/528 [02:25<07:50,  1.17s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 126/528 [02:26<07:48,  1.17s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 127/528 [02:27<07:48,  1.17s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 128/528 [02:28<07:47,  1.17s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  24%|██▍       | 129/528 [02:29<07:45,  1.17s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 130/528 [02:31<07:46,  1.17s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 131/528 [02:32<07:44,  1.17s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 132/528 [02:33<07:42,  1.17s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 133/528 [02:34<07:40,  1.17s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 134/528 [02:35<07:39,  1.17s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 135/528 [02:36<07:38,  1.17s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 136/528 [02:38<07:37,  1.17s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 137/528 [02:39<07:36,  1.17s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 138/528 [02:40<07:34,  1.17s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 139/528 [02:41<07:33,  1.17s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 140/528 [02:42<07:32,  1.17s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 141/528 [02:43<07:31,  1.17s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 142/528 [02:44<07:29,  1.17s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 143/528 [02:46<07:28,  1.17s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 144/528 [02:47<07:28,  1.17s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 145/528 [02:48<07:26,  1.17s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 146/528 [02:49<07:25,  1.17s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 147/528 [02:50<07:25,  1.17s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 148/528 [02:52<07:25,  1.17s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 149/528 [02:53<07:25,  1.17s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 150/528 [02:54<07:22,  1.17s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 151/528 [02:55<07:21,  1.17s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 152/528 [02:56<07:19,  1.17s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 153/528 [02:57<07:18,  1.17s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 154/528 [02:59<07:16,  1.17s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 155/528 [03:00<07:14,  1.17s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 156/528 [03:01<07:13,  1.16s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 157/528 [03:02<07:11,  1.16s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 158/528 [03:03<07:10,  1.16s/it]\u001b[A\n",
      "Iteration:  30%|███       | 159/528 [03:04<07:09,  1.16s/it]\u001b[A\n",
      "Iteration:  30%|███       | 160/528 [03:06<07:08,  1.16s/it]\u001b[A\n",
      "Iteration:  30%|███       | 161/528 [03:07<07:07,  1.16s/it]\u001b[A\n",
      "Iteration:  31%|███       | 162/528 [03:08<07:06,  1.17s/it]\u001b[A\n",
      "Iteration:  31%|███       | 163/528 [03:09<07:05,  1.17s/it]\u001b[A\n",
      "Iteration:  31%|███       | 164/528 [03:10<07:04,  1.17s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 165/528 [03:11<07:02,  1.16s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 166/528 [03:12<07:01,  1.16s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 167/528 [03:14<06:59,  1.16s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 168/528 [03:15<06:59,  1.16s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 169/528 [03:16<06:58,  1.16s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 170/528 [03:17<06:56,  1.16s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 171/528 [03:18<06:54,  1.16s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 172/528 [03:19<06:53,  1.16s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 173/528 [03:21<06:52,  1.16s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 174/528 [03:22<06:51,  1.16s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 175/528 [03:23<06:50,  1.16s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 176/528 [03:24<06:49,  1.16s/it]\u001b[A\n",
      "Iteration:  34%|███▎      | 177/528 [03:25<06:48,  1.16s/it]\u001b[A\n",
      "Iteration:  34%|███▎      | 178/528 [03:26<06:47,  1.16s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 179/528 [03:28<06:46,  1.16s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 180/528 [03:29<06:44,  1.16s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 181/528 [03:30<06:43,  1.16s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 182/528 [03:31<06:42,  1.16s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 183/528 [03:32<06:41,  1.16s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 184/528 [03:33<06:40,  1.16s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 185/528 [03:35<06:39,  1.16s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 186/528 [03:36<06:38,  1.16s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 187/528 [03:37<06:36,  1.16s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 188/528 [03:38<06:36,  1.17s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 189/528 [03:39<06:35,  1.17s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 190/528 [03:40<06:33,  1.17s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 191/528 [03:42<06:32,  1.16s/it]\u001b[A\n",
      "Iteration:  36%|███▋      | 192/528 [03:43<06:31,  1.16s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 193/528 [03:44<06:29,  1.16s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 194/528 [03:45<06:28,  1.16s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 195/528 [03:46<06:27,  1.16s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 196/528 [03:47<06:26,  1.16s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 197/528 [03:49<06:24,  1.16s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 198/528 [03:50<06:23,  1.16s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 199/528 [03:51<06:22,  1.16s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 200/528 [03:52<06:21,  1.16s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 201/528 [03:53<06:20,  1.16s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 202/528 [03:54<06:19,  1.16s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 203/528 [03:56<06:18,  1.16s/it]\u001b[A\n",
      "Iteration:  39%|███▊      | 204/528 [03:57<06:17,  1.16s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 205/528 [03:58<06:15,  1.16s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 206/528 [03:59<06:14,  1.16s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 207/528 [04:00<06:13,  1.16s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 208/528 [04:01<06:12,  1.16s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 209/528 [04:03<06:11,  1.16s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 210/528 [04:04<06:09,  1.16s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 211/528 [04:05<06:09,  1.16s/it]\u001b[A\n",
      "Iteration:  40%|████      | 212/528 [04:06<06:08,  1.16s/it]\u001b[A\n",
      "Iteration:  40%|████      | 213/528 [04:07<06:07,  1.17s/it]\u001b[A\n",
      "Iteration:  41%|████      | 214/528 [04:08<06:05,  1.17s/it]\u001b[A\n",
      "Iteration:  41%|████      | 215/528 [04:10<06:04,  1.16s/it]\u001b[A\n",
      "Iteration:  41%|████      | 216/528 [04:11<06:03,  1.16s/it]\u001b[A\n",
      "Iteration:  41%|████      | 217/528 [04:12<06:01,  1.16s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 218/528 [04:13<06:00,  1.16s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 219/528 [04:14<05:59,  1.16s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 220/528 [04:15<05:58,  1.16s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 221/528 [04:16<05:57,  1.16s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 222/528 [04:18<05:55,  1.16s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 223/528 [04:19<05:54,  1.16s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 224/528 [04:20<05:53,  1.16s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 225/528 [04:21<05:52,  1.16s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 226/528 [04:22<05:50,  1.16s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 227/528 [04:23<05:50,  1.16s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 228/528 [04:25<05:49,  1.16s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 229/528 [04:26<05:47,  1.16s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 230/528 [04:27<05:46,  1.16s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 231/528 [04:28<05:45,  1.16s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 232/528 [04:29<05:44,  1.16s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 233/528 [04:30<05:43,  1.17s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 234/528 [04:32<05:42,  1.17s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 235/528 [04:33<05:41,  1.17s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 236/528 [04:34<05:40,  1.17s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 237/528 [04:35<05:39,  1.17s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 238/528 [04:36<05:38,  1.17s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 239/528 [04:37<05:37,  1.17s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 240/528 [04:39<05:35,  1.17s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 241/528 [04:40<05:34,  1.17s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 242/528 [04:41<05:33,  1.17s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 243/528 [04:42<05:32,  1.17s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 244/528 [04:43<05:31,  1.17s/it]\u001b[A\n",
      "Iteration:  46%|████▋     | 245/528 [04:44<05:30,  1.17s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 246/528 [04:46<05:29,  1.17s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 247/528 [04:47<05:28,  1.17s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 248/528 [04:48<05:26,  1.17s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 249/528 [04:49<05:25,  1.17s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 250/528 [04:50<05:24,  1.17s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 251/528 [04:51<05:24,  1.17s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 252/528 [04:53<05:24,  1.17s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 253/528 [04:54<05:22,  1.17s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 254/528 [04:55<05:20,  1.17s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 255/528 [04:56<05:19,  1.17s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 256/528 [04:57<05:17,  1.17s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  49%|████▊     | 257/528 [04:58<05:16,  1.17s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 258/528 [05:00<05:14,  1.17s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 259/528 [05:01<05:13,  1.17s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 260/528 [05:02<05:15,  1.18s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 261/528 [05:03<05:16,  1.19s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 262/528 [05:04<05:16,  1.19s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 263/528 [05:06<05:16,  1.19s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 264/528 [05:07<05:15,  1.19s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 265/528 [05:08<05:13,  1.19s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 266/528 [05:09<05:11,  1.19s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 267/528 [05:10<05:14,  1.20s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 268/528 [05:12<05:14,  1.21s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 269/528 [05:13<05:11,  1.20s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 270/528 [05:14<05:03,  1.18s/it]\u001b[A\n",
      "Iteration:  51%|█████▏    | 271/528 [05:15<04:57,  1.16s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 272/528 [05:16<05:04,  1.19s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 273/528 [05:18<05:05,  1.20s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 274/528 [05:19<05:02,  1.19s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 275/528 [05:20<04:54,  1.16s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 276/528 [05:21<04:50,  1.15s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 277/528 [05:22<04:45,  1.14s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 278/528 [05:23<04:42,  1.13s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 279/528 [05:24<04:48,  1.16s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 280/528 [05:26<04:51,  1.18s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 281/528 [05:27<04:52,  1.18s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 282/528 [05:28<04:45,  1.16s/it]\u001b[A\n",
      "Iteration:  54%|█████▎    | 283/528 [05:29<04:39,  1.14s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 284/528 [05:30<04:36,  1.13s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 285/528 [05:31<04:33,  1.12s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 286/528 [05:32<04:29,  1.12s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 287/528 [05:33<04:27,  1.11s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 288/528 [05:35<04:25,  1.10s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 289/528 [05:36<04:22,  1.10s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 290/528 [05:37<04:23,  1.11s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 291/528 [05:38<04:23,  1.11s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 292/528 [05:39<04:20,  1.10s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 293/528 [05:40<04:17,  1.10s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 294/528 [05:41<04:16,  1.09s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 295/528 [05:42<04:18,  1.11s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 296/528 [05:43<04:15,  1.10s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 297/528 [05:44<04:14,  1.10s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 298/528 [05:46<04:12,  1.10s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 299/528 [05:47<04:13,  1.11s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 300/528 [05:48<04:13,  1.11s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 301/528 [05:49<04:10,  1.10s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 302/528 [05:50<04:11,  1.11s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 303/528 [05:51<04:18,  1.15s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 304/528 [05:52<04:21,  1.17s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 305/528 [05:54<04:15,  1.15s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 306/528 [05:55<04:15,  1.15s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 307/528 [05:56<04:12,  1.14s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 308/528 [05:57<04:08,  1.13s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 309/528 [05:58<04:05,  1.12s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 310/528 [05:59<04:03,  1.11s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 311/528 [06:00<04:00,  1.11s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 312/528 [06:01<04:00,  1.11s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 313/528 [06:02<04:00,  1.12s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 314/528 [06:04<04:02,  1.13s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 315/528 [06:05<04:03,  1.14s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 316/528 [06:06<04:00,  1.13s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 317/528 [06:07<03:56,  1.12s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 318/528 [06:08<03:57,  1.13s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 319/528 [06:09<03:56,  1.13s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 320/528 [06:11<04:00,  1.16s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 321/528 [06:12<04:03,  1.18s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 322/528 [06:13<03:58,  1.16s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 323/528 [06:14<03:52,  1.13s/it]\u001b[A\n",
      "Iteration:  61%|██████▏   | 324/528 [06:15<03:48,  1.12s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 325/528 [06:16<03:48,  1.12s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 326/528 [06:17<03:45,  1.11s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 327/528 [06:18<03:42,  1.11s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 328/528 [06:19<03:41,  1.11s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 329/528 [06:21<03:39,  1.10s/it]\u001b[A\n",
      "Iteration:  62%|██████▎   | 330/528 [06:22<03:38,  1.10s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 331/528 [06:23<03:39,  1.11s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 332/528 [06:24<03:39,  1.12s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 333/528 [06:25<03:40,  1.13s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 334/528 [06:26<03:38,  1.13s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 335/528 [06:27<03:40,  1.14s/it]\u001b[A\n",
      "Iteration:  64%|██████▎   | 336/528 [06:28<03:35,  1.12s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 337/528 [06:30<03:32,  1.11s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 338/528 [06:31<03:30,  1.11s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 339/528 [06:32<03:27,  1.10s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 340/528 [06:33<03:25,  1.10s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 341/528 [06:34<03:24,  1.09s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 342/528 [06:35<03:22,  1.09s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 343/528 [06:36<03:21,  1.09s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 344/528 [06:37<03:19,  1.09s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 345/528 [06:38<03:18,  1.09s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 346/528 [06:39<03:17,  1.09s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 347/528 [06:40<03:16,  1.08s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 348/528 [06:41<03:15,  1.08s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 349/528 [06:43<03:13,  1.08s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 350/528 [06:44<03:12,  1.08s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 351/528 [06:45<03:11,  1.08s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 352/528 [06:46<03:10,  1.08s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 353/528 [06:47<03:09,  1.08s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 354/528 [06:48<03:08,  1.08s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 355/528 [06:49<03:07,  1.08s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 356/528 [06:50<03:06,  1.08s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 357/528 [06:51<03:06,  1.09s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 358/528 [06:52<03:05,  1.09s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 359/528 [06:53<03:04,  1.09s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 360/528 [06:54<03:03,  1.09s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 361/528 [06:56<03:01,  1.09s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 362/528 [06:57<03:00,  1.09s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 363/528 [06:58<02:59,  1.09s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 364/528 [06:59<02:58,  1.09s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 365/528 [07:00<02:57,  1.09s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 366/528 [07:01<02:55,  1.09s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 367/528 [07:02<02:54,  1.09s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 368/528 [07:03<02:53,  1.09s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 369/528 [07:04<02:52,  1.09s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 370/528 [07:05<02:51,  1.09s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 371/528 [07:06<02:50,  1.08s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 372/528 [07:08<02:49,  1.08s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 373/528 [07:09<02:48,  1.08s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 374/528 [07:10<02:46,  1.08s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 375/528 [07:11<02:45,  1.08s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 376/528 [07:12<02:45,  1.09s/it]\u001b[A\n",
      "Iteration:  71%|███████▏  | 377/528 [07:13<02:43,  1.09s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 378/528 [07:14<02:42,  1.09s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 379/528 [07:15<02:41,  1.09s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 380/528 [07:16<02:40,  1.09s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 381/528 [07:17<02:39,  1.09s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 382/528 [07:18<02:38,  1.09s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 383/528 [07:19<02:37,  1.09s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 384/528 [07:21<02:36,  1.09s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  73%|███████▎  | 385/528 [07:22<02:35,  1.09s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 386/528 [07:23<02:34,  1.09s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 387/528 [07:24<02:32,  1.08s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 388/528 [07:25<02:31,  1.08s/it]\u001b[A\n",
      "Iteration:  74%|███████▎  | 389/528 [07:26<02:30,  1.08s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 390/528 [07:27<02:29,  1.08s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 391/528 [07:28<02:28,  1.08s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 392/528 [07:29<02:27,  1.08s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 393/528 [07:30<02:26,  1.08s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 394/528 [07:31<02:25,  1.08s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 395/528 [07:32<02:24,  1.08s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 396/528 [07:34<02:23,  1.08s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 397/528 [07:35<02:21,  1.08s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 398/528 [07:36<02:20,  1.08s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 399/528 [07:37<02:19,  1.08s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 400/528 [07:38<02:18,  1.08s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 401/528 [07:39<02:17,  1.08s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 402/528 [07:40<02:16,  1.08s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 403/528 [07:41<02:15,  1.08s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 404/528 [07:42<02:14,  1.08s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 405/528 [07:43<02:13,  1.08s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 406/528 [07:44<02:12,  1.08s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 407/528 [07:45<02:11,  1.08s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 408/528 [07:47<02:10,  1.08s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 409/528 [07:48<02:09,  1.08s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 410/528 [07:49<02:07,  1.08s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 411/528 [07:50<02:06,  1.09s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 412/528 [07:51<02:05,  1.09s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 413/528 [07:52<02:04,  1.09s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 414/528 [07:53<02:03,  1.09s/it]\u001b[A\n",
      "Iteration:  79%|███████▊  | 415/528 [07:54<02:02,  1.09s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 416/528 [07:55<02:01,  1.09s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 417/528 [07:56<02:00,  1.09s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 418/528 [07:57<01:59,  1.09s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 419/528 [07:59<01:58,  1.09s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 420/528 [08:00<01:57,  1.08s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 421/528 [08:01<01:56,  1.09s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 422/528 [08:02<01:55,  1.08s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 423/528 [08:03<01:53,  1.09s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 424/528 [08:04<01:52,  1.09s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 425/528 [08:05<01:51,  1.09s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 426/528 [08:06<01:50,  1.09s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 427/528 [08:07<01:49,  1.09s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 428/528 [08:08<01:48,  1.09s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 429/528 [08:09<01:47,  1.09s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 430/528 [08:10<01:46,  1.08s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 431/528 [08:12<01:45,  1.08s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 432/528 [08:13<01:44,  1.08s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 433/528 [08:14<01:42,  1.08s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 434/528 [08:15<01:41,  1.08s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 435/528 [08:16<01:40,  1.08s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 436/528 [08:17<01:39,  1.08s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 437/528 [08:18<01:38,  1.08s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 438/528 [08:19<01:37,  1.08s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 439/528 [08:20<01:36,  1.08s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 440/528 [08:21<01:35,  1.08s/it]\u001b[A\n",
      "Iteration:  84%|████████▎ | 441/528 [08:22<01:34,  1.08s/it]\u001b[A\n",
      "Iteration:  84%|████████▎ | 442/528 [08:23<01:33,  1.08s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 443/528 [08:25<01:32,  1.08s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 444/528 [08:26<01:31,  1.08s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 445/528 [08:27<01:29,  1.08s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 446/528 [08:28<01:28,  1.08s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 447/528 [08:29<01:27,  1.08s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 448/528 [08:30<01:26,  1.08s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 449/528 [08:31<01:25,  1.08s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 450/528 [08:32<01:24,  1.08s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 451/528 [08:33<01:23,  1.08s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 452/528 [08:34<01:22,  1.08s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 453/528 [08:35<01:21,  1.09s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 454/528 [08:36<01:20,  1.09s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 455/528 [08:38<01:19,  1.09s/it]\u001b[A\n",
      "Iteration:  86%|████████▋ | 456/528 [08:39<01:18,  1.08s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 457/528 [08:40<01:17,  1.08s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 458/528 [08:41<01:15,  1.08s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 459/528 [08:42<01:14,  1.08s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 460/528 [08:43<01:13,  1.08s/it]\u001b[A\n",
      "Iteration:  87%|████████▋ | 461/528 [08:44<01:12,  1.08s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 462/528 [08:45<01:11,  1.08s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 463/528 [08:46<01:10,  1.08s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 464/528 [08:47<01:09,  1.08s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 465/528 [08:48<01:08,  1.08s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 466/528 [08:49<01:07,  1.08s/it]\u001b[A\n",
      "Iteration:  88%|████████▊ | 467/528 [08:51<01:06,  1.09s/it]\u001b[A\n",
      "Iteration:  89%|████████▊ | 468/528 [08:52<01:05,  1.09s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 469/528 [08:53<01:04,  1.09s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 470/528 [08:54<01:03,  1.09s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 471/528 [08:55<01:02,  1.09s/it]\u001b[A\n",
      "Iteration:  89%|████████▉ | 472/528 [08:56<01:00,  1.09s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 473/528 [08:57<00:59,  1.09s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 474/528 [08:58<00:58,  1.09s/it]\u001b[A\n",
      "Iteration:  90%|████████▉ | 475/528 [08:59<00:57,  1.08s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 476/528 [09:00<00:56,  1.09s/it]\u001b[A\n",
      "Iteration:  90%|█████████ | 477/528 [09:01<00:55,  1.08s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 478/528 [09:03<00:54,  1.08s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 479/528 [09:04<00:53,  1.08s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 480/528 [09:05<00:52,  1.08s/it]\u001b[A\n",
      "Iteration:  91%|█████████ | 481/528 [09:06<00:50,  1.08s/it]\u001b[A\n",
      "Iteration:  91%|█████████▏| 482/528 [09:07<00:49,  1.08s/it]\u001b[A\n",
      "Iteration:  91%|█████████▏| 483/528 [09:08<00:48,  1.09s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 484/528 [09:09<00:47,  1.08s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 485/528 [09:10<00:46,  1.08s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 486/528 [09:11<00:45,  1.08s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 487/528 [09:12<00:44,  1.08s/it]\u001b[A\n",
      "Iteration:  92%|█████████▏| 488/528 [09:13<00:43,  1.08s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 489/528 [09:14<00:42,  1.08s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 490/528 [09:16<00:41,  1.08s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 491/528 [09:17<00:40,  1.08s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 492/528 [09:18<00:38,  1.08s/it]\u001b[A\n",
      "Iteration:  93%|█████████▎| 493/528 [09:19<00:37,  1.08s/it]\u001b[A\n",
      "Iteration:  94%|█████████▎| 494/528 [09:20<00:36,  1.08s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 495/528 [09:21<00:35,  1.08s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 496/528 [09:22<00:34,  1.08s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 497/528 [09:23<00:33,  1.08s/it]\u001b[A\n",
      "Iteration:  94%|█████████▍| 498/528 [09:24<00:32,  1.08s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 499/528 [09:25<00:31,  1.08s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 500/528 [09:26<00:30,  1.08s/it]\u001b[A\n",
      "Iteration:  95%|█████████▍| 501/528 [09:27<00:29,  1.08s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 502/528 [09:29<00:28,  1.08s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 503/528 [09:30<00:27,  1.08s/it]\u001b[A\n",
      "Iteration:  95%|█████████▌| 504/528 [09:31<00:26,  1.08s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 505/528 [09:32<00:24,  1.08s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 506/528 [09:33<00:23,  1.08s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 507/528 [09:34<00:22,  1.08s/it]\u001b[A\n",
      "Iteration:  96%|█████████▌| 508/528 [09:35<00:21,  1.08s/it]\u001b[A\n",
      "Iteration:  96%|█████████▋| 509/528 [09:36<00:20,  1.08s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 510/528 [09:37<00:19,  1.08s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 511/528 [09:38<00:18,  1.08s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 512/528 [09:39<00:17,  1.08s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  97%|█████████▋| 513/528 [09:40<00:16,  1.08s/it]\u001b[A\n",
      "Iteration:  97%|█████████▋| 514/528 [09:42<00:15,  1.08s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 515/528 [09:43<00:14,  1.08s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 516/528 [09:44<00:13,  1.08s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 517/528 [09:45<00:11,  1.08s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 518/528 [09:46<00:10,  1.08s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 519/528 [09:47<00:09,  1.08s/it]\u001b[A\n",
      "Iteration:  98%|█████████▊| 520/528 [09:48<00:08,  1.08s/it]\u001b[A\n",
      "Iteration:  99%|█████████▊| 521/528 [09:49<00:07,  1.08s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 522/528 [09:50<00:06,  1.08s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 523/528 [09:51<00:05,  1.09s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 524/528 [09:52<00:04,  1.08s/it]\u001b[A\n",
      "Iteration:  99%|█████████▉| 525/528 [09:53<00:03,  1.08s/it]\u001b[A\n",
      "Iteration: 100%|█████████▉| 526/528 [09:55<00:02,  1.08s/it]\u001b[A\n",
      "Iteration: 100%|█████████▉| 527/528 [09:56<00:01,  1.08s/it]\u001b[A\n",
      "Iteration: 100%|██████████| 528/528 [09:56<00:00,  1.13s/it]\u001b[A\n",
      "Epoch: 100%|██████████| 1/1 [09:56<00:00, 596.51s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loss_set = []\n",
    "\n",
    "model.train()\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "        if torch.cuda.is_available():\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        train_loss_set.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([])\n",
    "y_true = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 59/59 [00:41<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "outputs = []\n",
    "\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(validation_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "\n",
    "    current_out = np.argmax(logits, axis=1)\n",
    "\n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "    y_pred = np.concatenate((y_pred,np.argmax(logits, axis=1)),axis=None)\n",
    "    y_true = np.concatenate((y_true,label_ids),axis=None)\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "\n",
    "loss = tr_loss/nb_tr_steps\n",
    "\n",
    "result = {'eval_loss': eval_loss,\n",
    "          'eval_accuracy': eval_accuracy,\n",
    "          'train_loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.26039724708613704,\n",
       " 'eval_accuracy': 0.8946666666666667,\n",
       " 'train_loss': 0.23427869711127697}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['Pos', 'Neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Pos       0.90      0.89      0.89      3737\n",
      "         Neg       0.89      0.90      0.90      3763\n",
      "\n",
      "    accuracy                           0.89      7500\n",
      "   macro avg       0.89      0.89      0.89      7500\n",
      "weighted avg       0.89      0.89      0.89      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
